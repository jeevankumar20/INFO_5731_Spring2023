{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third In-class-exercise (2/28/2023, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease write you answer here:\\nAn interesting text classification task could be classifying customer reviews of a product into positive, negative or neutral sentiments.\\nThis task is useful for businesses to understand customer feedback and improve their product or service.\\n\\nTo build a machine learning model for this task, the following features could be helpful:\\n\\nBag of Words (BoW) - A BoW approach can be used to represent each review as a vector of word frequencies.\\nThis is a commonly used feature in text classification tasks and it can help capture important keywords and their frequency in a review.\\n\\nN-grams - N-grams refer to contiguous sequences of n words in a text.\\nThey can be useful to capture the context of words in a review.\\nFor example, bigrams (n=2) can capture phrases like \"customer service\" which can be a strong indicator of sentiment.\\n\\nPart-of-speech (POS) tags - POS tags can help identify the role of each word in a sentence such as noun, verb, adjective etc.\\nThis information can be used to capture grammatical features of a review and their impact on sentiment.\\n\\nSentiment lexicons - Sentiment lexicons are lists of words with their associated sentiment polarity (positive or negative).\\nThey can be useful for identifying the sentiment of a review based on the presence of certain sentiment words.\\n\\nNamed entities - Named entities refer to specific names of people, organizations, and places in a text.\\nThey can be useful for identifying the subject of a review and understanding how the sentiment relates to specific entities.\\n\\nBy using a combination of these features, the machine learning model can learn to distinguish between positive, negative, and neutral sentiments in customer reviews of a product.\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "An interesting text classification task could be classifying customer reviews of a product into positive, negative or neutral sentiments.\n",
    "This task is useful for businesses to understand customer feedback and improve their product or service.\n",
    "\n",
    "To build a machine learning model for this task, the following features could be helpful:\n",
    "\n",
    "Bag of Words (BoW) - A BoW approach can be used to represent each review as a vector of word frequencies.\n",
    "This is a commonly used feature in text classification tasks and it can help capture important keywords and their frequency in a review.\n",
    "\n",
    "N-grams - N-grams refer to contiguous sequences of n words in a text.\n",
    "They can be useful to capture the context of words in a review.\n",
    "For example, bigrams (n=2) can capture phrases like \"customer service\" which can be a strong indicator of sentiment.\n",
    "\n",
    "Part-of-speech (POS) tags - POS tags can help identify the role of each word in a sentence such as noun, verb, adjective etc.\n",
    "This information can be used to capture grammatical features of a review and their impact on sentiment.\n",
    "\n",
    "Sentiment lexicons - Sentiment lexicons are lists of words with their associated sentiment polarity (positive or negative).\n",
    "They can be useful for identifying the sentiment of a review based on the presence of certain sentiment words.\n",
    "\n",
    "Named entities - Named entities refer to specific names of people, organizations, and places in a text.\n",
    "They can be useful for identifying the subject of a review and understanding how the sentiment relates to specific entities.\n",
    "\n",
    "By using a combination of these features, the machine learning model can learn to distinguish between positive, negative, and neutral sentiments in customer reviews of a product.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words features:\n",
      "['beautiful', 'book', 'feel', 'good', 'love', 'movie', 'neutral', 'restaurant', 'service', 'terrible', 'today', 'topic', 'weather']\n",
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "N-gram features:\n",
      "['beautiful today', 'book good', 'feel neutral', 'love movie', 'neutral topic', 'restaurant terrible', 'terrible service', 'weather beautiful']\n",
      "[[0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0]\n",
      " [1 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]]\n",
      "\n",
      "Part-of-speech (POS) features:\n",
      "[{'PRP': 1, 'VBP': 1, 'DT': 1, 'NN': 1, '.': 1}, {'DT': 1, 'NN': 2, 'VBZ': 1, 'JJ': 1, '.': 1}, {'DT': 1, 'NN': 2, 'VBZ': 1, 'JJ': 1, '.': 1}, {'PRP': 1, 'VBP': 1, 'JJ': 1, 'IN': 1, 'DT': 1, 'NN': 1, '.': 1}, {'DT': 1, 'NN': 1, 'VBD': 1, 'RB': 1, 'JJ': 1, '.': 1}]\n",
      "\n",
      "Sentiment lexicon features:\n",
      "[{'neg': 0.0, 'neu': 0.308, 'pos': 0.692, 'compound': 0.6696}, {'neg': 0.437, 'neu': 0.563, 'pos': 0.0, 'compound': -0.4767}, {'neg': 0.0, 'neu': 0.506, 'pos': 0.494, 'compound': 0.5994}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.376, 'neu': 0.624, 'pos': 0.0, 'compound': -0.3412}]\n",
      "\n",
      "Syntax features:\n",
      "[{'I love this movie !': True}, {'This restaurant has terrible service .': True}, {'The weather is beautiful today .': True}, {'I feel neutral about this topic .': True}, {'The book was not good .': True}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "texts = ['I love this movie!','This restaurant has terrible service.','The weather is beautiful today.','I feel neutral about this topic.','The book was not good.']\n",
    "\n",
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Bag-of-words features\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_features = bow_vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# N-gram features\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngram_features = ngram_vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Part-of-speech (POS) features\n",
    "pos_tagged_texts = [nltk.pos_tag(nltk.word_tokenize(text)) for text in texts]\n",
    "\n",
    "pos_features = []\n",
    "for tagged_text in pos_tagged_texts:\n",
    "    feature_dict = {}\n",
    "    for token, tag in tagged_text:\n",
    "        feature_dict[tag] = feature_dict.get(tag, 0) + 1\n",
    "    pos_features.append(feature_dict)\n",
    "\n",
    "# Sentiment lexicon features\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_lexicon_features = [sia.polarity_scores(text) for text in texts]\n",
    "\n",
    "#Syntax Features\n",
    "syntax_features = []\n",
    "for text in texts:\n",
    "    parse_tree = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "    feature_dict = {}\n",
    "    for subtree in parse_tree.subtrees(filter=lambda t: t.label() == 'S'):\n",
    "        subtree_words = [word for word, tag in subtree.leaves()]\n",
    "        feature_dict[' '.join(subtree_words)] = True\n",
    "    syntax_features.append(feature_dict)\n",
    "\n",
    "\n",
    "# Print the extracted features\n",
    "print('Bag-of-words features:')\n",
    "print(bow_vectorizer.get_feature_names())\n",
    "print(bow_features.toarray())\n",
    "print('')\n",
    "\n",
    "print('N-gram features:')\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(ngram_features.toarray())\n",
    "print('')\n",
    "\n",
    "print('Part-of-speech (POS) features:')\n",
    "print(pos_features)\n",
    "print('')\n",
    "\n",
    "print('Sentiment lexicon features:')\n",
    "print(sentiment_lexicon_features)\n",
    "print('')\n",
    "\n",
    "print('Syntax features:')\n",
    "print(syntax_features)\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square scores for all features:\n",
      "flesch_score 19.385948616600796\n",
      "attention 4.000000000000001\n",
      "capture 4.000000000000001\n",
      "either 4.000000000000001\n",
      "okay 4.000000000000001\n",
      "really 4.000000000000001\n",
      "wasnt 4.000000000000001\n",
      "attention wasnt 4.000000000000001\n",
      "book okay 4.000000000000001\n",
      "capture attention 4.000000000000001\n",
      "didnt really 4.000000000000001\n",
      "okay didnt 4.000000000000001\n",
      "really capture 4.000000000000001\n",
      "terrible either 4.000000000000001\n",
      "wasnt terrible 4.000000000000001\n",
      "attention 4.000000000000001\n",
      "capture 4.000000000000001\n",
      "okay 4.000000000000001\n",
      "really 4.000000000000001\n",
      "wasnt 4.000000000000001\n",
      "book 1.75\n",
      "didnt 1.75\n",
      "terrible 1.75\n",
      "book 1.75\n",
      "didnt 1.75\n",
      "terrible 1.75\n",
      "advertised 1.5\n",
      "atmosphere 1.5\n",
      "buying 1.5\n",
      "characters 1.5\n",
      "cold 1.5\n",
      "delicious 1.5\n",
      "engaging 1.5\n",
      "excellent 1.5\n",
      "expecting 1.5\n",
      "experience 1.5\n",
      "highly 1.5\n",
      "perfect 1.5\n",
      "product 1.5\n",
      "recommend 1.5\n",
      "regret 1.5\n",
      "slow 1.5\n",
      "wellwritten 1.5\n",
      "work 1.5\n",
      "atmosphere perfect 1.5\n",
      "book wellwritten 1.5\n",
      "buying product 1.5\n",
      "characters engaging 1.5\n",
      "delicious atmosphere 1.5\n",
      "didnt work 1.5\n",
      "excellent food 1.5\n",
      "expecting didnt 1.5\n",
      "experience restaurant 1.5\n",
      "food cold 1.5\n",
      "food delicious 1.5\n",
      "highly recommend 1.5\n",
      "product expecting 1.5\n",
      "recommend book 1.5\n",
      "regret buying 1.5\n",
      "restaurant excellent 1.5\n",
      "restaurant service 1.5\n",
      "service restaurant 1.5\n",
      "service slow 1.5\n",
      "slow food 1.5\n",
      "terrible experience 1.5\n",
      "wellwritten characters 1.5\n",
      "work advertised 1.5\n",
      "advertised 1.5\n",
      "atmosphere 1.5\n",
      "buying 1.5\n",
      "characters 1.5\n",
      "cold 1.5\n",
      "delicious 1.5\n",
      "engaging 1.5\n",
      "excellent 1.5\n",
      "expecting 1.5\n",
      "experience 1.5\n",
      "highly 1.5\n",
      "perfect 1.5\n",
      "product 1.5\n",
      "recommend 1.5\n",
      "regret 1.5\n",
      "slow 1.5\n",
      "wellwritten 1.5\n",
      "work 1.5\n",
      "pos 1.2604404973357013\n",
      "neg 0.9885000000000003\n",
      "food 0.5\n",
      "restaurant 0.5\n",
      "service 0.5\n",
      "food 0.5\n",
      "restaurant 0.5\n",
      "service 0.5\n",
      "neu 0.20897567873303177\n",
      "compound 0.17970173133223405\n",
      "smog_score nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeeva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.sparse import hstack\n",
    "import textstat\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Sample data\n",
    "mytext = [\"The service at this restaurant was excellent. The food was delicious and the atmosphere was perfect.\",\n",
    "          \"I had a terrible experience at this restaurant. The service was slow and the food was cold.\",\n",
    "          \"I highly recommend this book. It's well-written and the characters are engaging.\",\n",
    "          \"This book was just okay. It didn't really capture my attention, but it wasn't terrible either.\",\n",
    "          \"I regret buying this product. It was not what I was expecting and it didn't work as advertised.\"]\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "mytext_preprocessed = [preprocess_text(text) for text in mytext]\n",
    "\n",
    "# Target class labels\n",
    "labels = np.array([1, 0, 1, 2, 0])  # 1 for positive, 0 for negative, 2 for neutral\n",
    "\n",
    "# Bag of Words feature extraction\n",
    "vectorizer_bow = CountVectorizer()\n",
    "bow_features = vectorizer_bow.fit_transform(mytext_preprocessed)\n",
    "\n",
    "# N-Grams feature extraction\n",
    "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_features = vectorizer_ngrams.fit_transform(mytext_preprocessed)\n",
    "\n",
    "# Part-of-speech feature extraction\n",
    "pos_vectorizer = CountVectorizer(token_pattern=r'\\b\\w\\w+\\b|!|\\?|\\\"|\\'', ngram_range=(1,1), analyzer='word', \n",
    "                                 stop_words='english')\n",
    "pos_features = pos_vectorizer.fit_transform(mytext_preprocessed)\n",
    "\n",
    "# Sentiment Lexicon feature extraction\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "lexicon_features = []\n",
    "for doc in mytext_preprocessed:\n",
    "    vs = analyzer.polarity_scores(doc)\n",
    "    # apply non-negative transformation\n",
    "    lexicon_features.append([abs(vs['neg']), abs(vs['neu']), abs(vs['pos']), abs(vs['compound'])])\n",
    "lexicon_features = np.array(lexicon_features)\n",
    "\n",
    "# Readability feature extraction\n",
    "readability_features = []\n",
    "for doc in mytext_preprocessed:\n",
    "    flesch_score = textstat.flesch_reading_ease(doc)\n",
    "    smog_score = textstat.smog_index(doc)\n",
    "    # apply non-negative transformation\n",
    "    readability_features.append([abs(flesch_score), abs(smog_score)])\n",
    "readability_features = np.array(readability_features)\n",
    "\n",
    "# Concatenate all features horizontally\n",
    "features = hstack((bow_features, ngram_features, pos_features, lexicon_features, readability_features))\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer_bow.get_feature_names() + vectorizer_ngrams.get_feature_names() + pos_vectorizer.get_feature_names() + ['neg', 'neu', 'pos', 'compound'] + ['flesch_score', 'smog_score']\n",
    "\n",
    "# Chi-Square feature selection\n",
    "chi2_scores, _ = chi2(features, labels)\n",
    "feature_scores = list(zip(feature_names, chi2_scores))\n",
    "feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Chi-Square scores for all features:\")\n",
    "for feature, score in feature_scores:\n",
    "    print(feature, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Rank | Text                                                                                             |   Similarity |\n",
      "|--------+--------------------------------------------------------------------------------------------------+--------------|\n",
      "|      1 | This product is just okay. It wasn't great, but it wasn't terrible either.                       |       0.5735 |\n",
      "|      2 | I was really disappointed with this product. It didn't work as advertised.                       |       0.4877 |\n",
      "|      3 | I love this product so much! It has made my life so much easier.                                 |       0.4121 |\n",
      "|      4 | This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone. |       0.3285 |\n",
      "|      5 | I would never buy this product again. It was a complete waste of money.                          |       0.294  |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the BERT model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Tokenize text data and query\n",
    "texts = [\"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\", \n",
    "            \"I was really disappointed with this product. It didn't work as advertised.\", \n",
    "            \"I love this product so much! It has made my life so much easier.\",    \n",
    "            \"This product is just okay. It wasn't great, but it wasn't terrible either.\",  \n",
    "            \"I would never buy this product again. It was a complete waste of money.\"]\n",
    "         \n",
    "query = \"restaurant service was great, but the food was cold\"\n",
    "\n",
    "# Compute the embeddings for the query and texts\n",
    "query_embedding = model.encode([query])[0]\n",
    "text_embeddings = model.encode(texts)\n",
    "\n",
    "# Compute the cosine similarities between the query and texts\n",
    "similarities = cosine_similarity([query_embedding], text_embeddings)[0]\n",
    "\n",
    "# Rank the texts based on their similarity to the query\n",
    "from tabulate import tabulate\n",
    "\n",
    "table = []\n",
    "for i, (text, similarity) in enumerate(ranked_texts):\n",
    "    table.append([i+1, text, round(similarity, 4)])\n",
    "    \n",
    "print(tabulate(table, headers=['Rank', 'Text', 'Similarity'], tablefmt='orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
