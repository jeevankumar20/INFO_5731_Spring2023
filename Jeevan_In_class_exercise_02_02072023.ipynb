{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevankumar20/INFO_5731_Spring2023/blob/main/Jeevan_In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTZqTcewUzZH"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFj96QXpUzZJ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw0GRH1EUzZJ"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag6bODpFUzZK"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Some research questions that can be answered by the the bellow collected can be:\n",
        "\n",
        "1. What are the most highly rated movies of all time according to IMDb users?\n",
        "2. How has the average movie rating changed over time?\n",
        "3. Do highly rated movies tend to have higher gross revenue?\n",
        "4. Do movies with a specific rating certificate (e.g. G, PG, R) tend to receive higher ratings on IMDb?\n",
        "\n",
        ">The data is collected from IMDb website and the specific data I collected is:\n",
        " Movie title, year of release, IMDb rating, runtime, Certificate(G, PG, R), and gross revenue.\n",
        ">Web scraping is performed using a BeautifulSoup library to extract the data from the website.\n",
        ">The data is loaded into a dataframe and is saved to a csv file.s\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmMtUPhEUzZL"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp1OsJ5IUzZL",
        "outputId": "25191033-8ef7-4f28-9140-05f364b84fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Movie    Year Rating  Runtime  \\\n",
            "0                 The Shawshank Redemption  (1994)    9.3  142 min   \n",
            "1                            The Godfather  (1972)    9.2  175 min   \n",
            "2                          The Dark Knight  (2008)    9.0  152 min   \n",
            "3                         Schindler's List  (1993)    9.0  195 min   \n",
            "4                    The Godfather Part II  (1974)    9.0  202 min   \n",
            "..                                     ...     ...    ...      ...   \n",
            "995                      Good Will Hunting  (1997)    8.3  126 min   \n",
            "996                   Inglourious Basterds  (2009)    8.3  153 min   \n",
            "997                                   Heat  (1995)    8.3  170 min   \n",
            "998                    Requiem for a Dream  (2000)    8.3  102 min   \n",
            "999  Eternal Sunshine of the Spotless Mind  (2004)    8.3  108 min   \n",
            "\n",
            "    Certificate    Gross  \n",
            "0             R  2698695  \n",
            "1             R  1872940  \n",
            "2         PG-13  2672462  \n",
            "3             R  1364329  \n",
            "4             R  1280001  \n",
            "..          ...      ...  \n",
            "995           R   981811  \n",
            "996           R  1463012  \n",
            "997           R   661670  \n",
            "998           R   849142  \n",
            "999           R  1015794  \n",
            "\n",
            "[1000 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Request the page\n",
        "url = \"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&ref_=adv_prv\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Find all movie containers\n",
        "movie_containers = soup.find_all(\"div\", class_=\"lister-item mode-advanced\")\n",
        "\n",
        "# Create a list to store the movie information\n",
        "movies = []\n",
        "\n",
        "# Loop through each page\n",
        "for page in range(1, 11):\n",
        "    # Calculate the starting point of the movies for the current page\n",
        "    start = (page - 1) * 100 + 1\n",
        "\n",
        "    # Create the URL for the current page\n",
        "    url = f\"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start={start}&ref_=adv_nxt\"\n",
        "\n",
        "    # Loop through each movie container and extract the movie information\n",
        "    for container in movie_containers:\n",
        "        name = container.h3.a.text\n",
        "        year = container.h3.find(\"span\", class_=\"lister-item-year\").text\n",
        "        rating = container.strong.text\n",
        "        runtime = container.find(\"span\", class_=\"runtime\").text.strip()\n",
        "        movie_type = container.find(\"span\", class_=\"certificate\").text.strip() if container.find(\"span\", class_=\"certificate\") else None\n",
        "        gross_element = container.find(\"span\", attrs={\"name\": \"nv\"})\n",
        "        gross = gross_element[\"data-value\"] if gross_element else None\n",
        "\n",
        "    # Add the movie information to the list\n",
        "        movies.append([name, year, rating, runtime, movie_type, gross])\n",
        "\n",
        "# Convert the list of movies to a Pandas DataFrame\n",
        "df = pd.DataFrame(movies, columns=[\"Movie\", \"Year\", \"Rating\", \"Runtime\", \"Certificate\", \"Gross\"])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "\n",
        "df.to_csv(\"movies.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgrAftBiUzZN"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Mrk2f8UzZN",
        "outputId": "cc84320f-b547-4c11-940e-e9e3c0336e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  year  \\\n",
            "0  BEIR: A Heterogenous Benchmark for Zero-shot E...  2021   \n",
            "1                              Information Retrieval  2018   \n",
            "2  COIL: Revisit Exact Lexical Match in Informati...  2021   \n",
            "3  Pyserini: A Python Toolkit for Reproducible In...  2021   \n",
            "4  SPLADE v2: Sparse Lexical and Expansion Model ...  2021   \n",
            "\n",
            "                                               venue  \\\n",
            "0                    NeurIPS Datasets and Benchmarks   \n",
            "1                  Lecture Notes in Computer Science   \n",
            "2  North American Chapter of the Association for ...   \n",
            "3  Annual International ACM SIGIR Conference on R...   \n",
            "4                                              ArXiv   \n",
            "\n",
            "                                             journal  \\\n",
            "0      {'name': 'ArXiv', 'volume': 'abs/2104.08663'}   \n",
            "1  {'name': 'Canadian Medical Association journal...   \n",
            "2                             {'pages': '3030-3042'}   \n",
            "3  {'name': 'Proceedings of the 44th Internationa...   \n",
            "4      {'name': 'ArXiv', 'volume': 'abs/2109.10086'}   \n",
            "\n",
            "                             publicationTypes  \\\n",
            "0                            [JournalArticle]   \n",
            "1                            [JournalArticle]   \n",
            "2                [JournalArticle, Conference]   \n",
            "3  [JournalArticle, Book, Conference, Review]   \n",
            "4                            [JournalArticle]   \n",
            "\n",
            "                                             authors  \\\n",
            "0  [{'authorId': '47583894', 'name': 'Nandan Thak...   \n",
            "1  [{'authorId': '32293755', 'name': 'A. Dekhtyar'}]   \n",
            "2  [{'authorId': '49715441', 'name': 'Luyu Gao'},...   \n",
            "3  [{'authorId': '145580839', 'name': 'Jimmy J. L...   \n",
            "4  [{'authorId': '1630412772', 'name': 'Thibault ...   \n",
            "\n",
            "                                            abstract  \n",
            "0  Existing neural information retrieval (IR) mod...  \n",
            "1                                               None  \n",
            "2  Classical information retrieval systems such a...  \n",
            "3  Pyserini is a Python toolkit for reproducible ...  \n",
            "4  In neural Information Retrieval (IR), ongoing ...  \n",
            "                                                 title  year  \\\n",
            "995  Global monitoring of terrestrial chlorophyll f...  2013   \n",
            "996  DNA Fountain enables a robust and efficient st...  2016   \n",
            "997  Object lens: a “spreadsheet” for cooperative work  2018   \n",
            "998  Ground, Proximal, and Satellite Remote Sensing...  2019   \n",
            "999  Validation of Soil Moisture and Ocean Salinity...  2012   \n",
            "\n",
            "                                                 venue  \\\n",
            "995                                                      \n",
            "996                                            Science   \n",
            "997                                               TOIS   \n",
            "998                              Reviews of Geophysics   \n",
            "999  IEEE Transactions on Geoscience and Remote Sen...   \n",
            "\n",
            "                                               journal  publicationTypes  \\\n",
            "995  {'name': 'Atmospheric Measurement Techniques',...              None   \n",
            "996  {'name': 'Science', 'pages': '950 - 954', 'vol...  [JournalArticle]   \n",
            "997  {'name': 'ACM Trans. Inf. Syst.', 'pages': '33...  [JournalArticle]   \n",
            "998  {'name': 'Reviews of Geophysics', 'pages': '53...          [Review]   \n",
            "999  {'name': 'IEEE Transactions on Geoscience and ...  [JournalArticle]   \n",
            "\n",
            "                                               authors  \\\n",
            "995  [{'authorId': '145945528', 'name': 'J. Joiner'...   \n",
            "996  [{'authorId': '2422788', 'name': 'Yaniv Erlich...   \n",
            "997  [{'authorId': '2657449', 'name': 'Kum-Yew Lai'...   \n",
            "998  [{'authorId': '93370209', 'name': 'Ebrahim Bab...   \n",
            "999  [{'authorId': '1734680', 'name': 'T. Jackson'}...   \n",
            "\n",
            "                                              abstract  \n",
            "995  Abstract. Globally mapped terrestrial chloroph...  \n",
            "996  A reliable and efficient DNA storage architect...  \n",
            "997  Object Lens allows unsophisticated computer us...  \n",
            "998  Soil moisture (SM) is a key hydrologic state v...  \n",
            "999  Estimation of soil moisture at large scale has...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def collect_articles(query, num_articles, start_year, end_year):\n",
        "    articles = []\n",
        "    for i in range(0, num_articles, 100):\n",
        "        url = \"https://api.semanticscholar.org/graph/v1/paper/search?query={}&offset={}&limit=100&year={}-{}&fields=title,year,venue,journal,publicationTypes,authors,abstract\".format(query, i, start_year, end_year)\n",
        "        response = requests.get(url)\n",
        "        results = response.json()\n",
        "        if 'error' in results:\n",
        "            print(\"Results: \", results)\n",
        "            break\n",
        "        for result in results['data']:\n",
        "            article = {}\n",
        "            article['title'] = result.get('title', '')\n",
        "            article['year'] = result.get('year', '')\n",
        "            article['venue'] = result.get('venue', '')\n",
        "            article['journal'] = result.get('journal', '')\n",
        "            article['publicationTypes'] = result.get('publicationTypes', [])\n",
        "            article['authors'] = result.get('authors', [])\n",
        "            article['abstract'] = result.get('abstract', '')\n",
        "            articles.append(article)\n",
        "    return articles\n",
        "\n",
        "query = \"information retrieval\"\n",
        "num_articles = 1000\n",
        "start_year = 2012\n",
        "end_year = 2022\n",
        "articles = collect_articles(query, num_articles, start_year, end_year)\n",
        "\n",
        "df = pd.DataFrame(articles)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDZv_KEwUzZO"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u01iYUIHUzZO",
        "outputId": "22fc7663-14d4-4634-ce48-cace8df792ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           User_name         Posted_time  \\\n",
            "0      OlDer43624126 2023-02-10 03:30:20   \n",
            "1           dillon66 2023-02-10 03:30:20   \n",
            "2        grey_sister 2023-02-10 03:30:19   \n",
            "3    DeiberthMartin1 2023-02-10 03:30:19   \n",
            "4     GalloAnonymous 2023-02-10 03:30:19   \n",
            "..               ...                 ...   \n",
            "995         chaeisl_ 2023-02-10 03:25:16   \n",
            "996  PankajSaini0220 2023-02-10 03:25:16   \n",
            "997        bernKleen 2023-02-10 03:25:16   \n",
            "998    notalemming24 2023-02-10 03:25:16   \n",
            "999          averiyt 2023-02-10 03:25:16   \n",
            "\n",
            "                                                  Text  \n",
            "0    RT @Mippcivzla: 🗣️ ¡𝐄𝐒 𝐂𝐎𝐍𝐓𝐈𝐆𝐎!✍️🇻🇪\\nEl síndro...  \n",
            "1    RT @KanekoaTheGreat: FLASHBACK: YouTube censor...  \n",
            "2    RT @IsisWise: On February 26, 2020 Heavoil Tec...  \n",
            "3    RT @Mippcivzla: 🗣️ ¡𝐓𝐎𝐌𝐀 𝐏𝐑𝐄𝐂𝐀𝐔𝐂𝐈𝐎́𝐍!✍️🇻🇪\\nVen...  \n",
            "4    RT @ALEGRA1988: \"Las *v☠cunas*  Covid potencia...  \n",
            "..                                                 ...  \n",
            "995  RT @purpletwts: @touringdata @JYPETWICE $33.9M...  \n",
            "996  RT @DDIndialive: .@MoHFW_INDIA updated it's 'G...  \n",
            "997  RT @thereds8: Curious, how many of you had zer...  \n",
            "998  RT @kimguilfoyle: Project Veritas under James ...  \n",
            "999  RT @KanekoaTheGreat: FLASHBACK: YouTube censor...  \n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import tweepy\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = \"u7L1lnR7HN85dn1qnTFO1cegb\"\n",
        "consumer_secret = \"QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT\"\n",
        "access_token = \"1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO\"\n",
        "access_token_secret = \"gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce\"\n",
        "\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create the API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Search for tweets containing the keyword \"information retrieval\"\n",
        "tweets = tweepy.Cursor(api.search, q='covid', tweet_mode='extended').items(1000)\n",
        "\n",
        "# Collect the relevant information\n",
        "tweet_data = []\n",
        "for tweet in tweets:\n",
        "    user_name = tweet.user.screen_name\n",
        "    posted_time = tweet.created_at\n",
        "    text = tweet.full_text\n",
        "    \n",
        "    tweet_data.append([user_name, posted_time, text])\n",
        "\n",
        "# Convert the tweet data to a pandas DataFrame\n",
        "\n",
        "df = pd.DataFrame(tweet_data, columns=['User_name', 'Posted_time', 'Text'])\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsYmqSTqUzZP"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSH9JB2jUzZP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}